{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0TRap0ysv/fFSwByWKUY0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-QkBJOEZAvZ"
      },
      "source": [
        "## Implementing a Complete Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzGO1yOzZAvZ"
      },
      "source": [
        "The purpose of neural layers is to be stacked together to form a ***neural network*** able to perform non-linear predictions.\n",
        "\n",
        "Applying a ***gradient descent***, such a network can be trained to perform correct predictions (c.f. theory in Chapter 1). But for that, we need a ***loss*** function to evaluate the performance of the network (c.f. ***L2*** or ***cross-entropy*** losses introduced in Chapter 1), and we need to know how to *derive* all the operations performed by the network, to compute and propagate the gradients.\n",
        "\n",
        "In this section, we will present how a simple fully-connected neural network can be built. Let us assume we want our network to use the *sigmoid* function for the activation. We need to implement that function _and_ its derivative:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zdlrZsVZAva"
      },
      "source": [
        "def sigmoid(x):             # sigmoid function\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "\n",
        "\n",
        "def derivated_sigmoid(y):   # sigmoid derivative function\n",
        "    return y * (1 - y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNo41oEwZAva"
      },
      "source": [
        "Suppose we want to build a neural network for classification. We would use the *L2* or *cross-entropy* loss previously introduced. We should also implement them, along their derivative:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfkC2YLtZAva"
      },
      "source": [
        "def loss_L2(pred, target):             # L2 loss function\n",
        "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
        "\n",
        "\n",
        "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
        "    return 2 * (pred - target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eiyt3xxCZAva"
      },
      "source": [
        "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
        "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
        "\n",
        "\n",
        "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
        "    return (pred - target) / (pred * (1 - pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B9dkW_DZAva"
      },
      "source": [
        "As described in the book, we should now connect everything together, building a class able to connect multiple neural layers together, able to to feed-forward data through these layers and back-propagate the loss' gradients for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r58LJNRXZAva"
      },
      "source": [
        "class SimpleNetwork(object):\n",
        "    \"\"\"A simple fully-connected NN.\n",
        "    Args:\n",
        "        num_inputs (int): The input vector size / number of input values.\n",
        "        num_outputs (int): The output vector size.\n",
        "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
        "        activation_function (callable): The activation function for all the layers\n",
        "        derivated_activation_function (callable): The derivated activation function\n",
        "        loss_function (callable): The loss function to train this network\n",
        "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
        "    Attributes:\n",
        "        layers (list): The list of layers forming this simple network.\n",
        "        loss_function (callable): The loss function to train this network.\n",
        "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
        "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
        "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
        "        super().__init__()\n",
        "        # We build the list of layers composing the network, according to the provided arguments:\n",
        "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
        "        self.layers = [\n",
        "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], \n",
        "                                activation_function, derivated_activation_function)\n",
        "            for i in range(len(layer_sizes) - 1)]\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.derivated_loss_function = derivated_loss_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward the input vector through the layers, returning the output vector.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
        "        Returns:\n",
        "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
        "        \"\"\"\n",
        "        for layer in self.layers: # from the input layer to the output one\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Compute the output corresponding to input `x`, and return the index of the largest \n",
        "        output value.\n",
        "        Args:\n",
        "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
        "        Returns:\n",
        "            best_class (int): The predicted class ID.\n",
        "        \"\"\"\n",
        "        estimations = self.forward(x)\n",
        "        best_class = np.argmax(estimations)\n",
        "        return best_class\n",
        "\n",
        "    def backward(self, dL_dy):\n",
        "        \"\"\"\n",
        "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
        "        Args:\n",
        "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
        "        Returns:\n",
        "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
        "        \"\"\"\n",
        "        for layer in reversed(self.layers): # from the output layer to the input one\n",
        "            dL_dy = layer.backward(dL_dy)\n",
        "        return dL_dy\n",
        "\n",
        "    def optimize(self, epsilon):\n",
        "        \"\"\"\n",
        "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
        "        to be called before).\n",
        "        Args:\n",
        "            epsilon (float): The learning rate.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:             # the order doesn't matter here\n",
        "            layer.optimize(epsilon)\n",
        "\n",
        "    def evaluate_accuracy(self, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
        "        Args:\n",
        "            X_val (ndarray): The input validation dataset.\n",
        "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
        "        Returns:\n",
        "            accuracy (float): The accuracy of the network \n",
        "                              (= number of correct predictions/dataset size).\n",
        "        \"\"\"\n",
        "        num_corrects = 0\n",
        "        for i in range(len(X_val)):\n",
        "            pred_class = self.predict(X_val[i])\n",
        "            if pred_class == y_val[i]:\n",
        "                num_corrects += 1\n",
        "        return num_corrects / len(X_val)\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
        "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
        "        \"\"\"\n",
        "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
        "        Args:\n",
        "            X_train (ndarray): The input training dataset.\n",
        "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
        "            X_val (ndarray): The input validation dataset.\n",
        "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
        "            batch_size (int): The mini-batch size.\n",
        "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
        "            learning_rate (float): The learning rate to scale the derivatives.\n",
        "            print_frequency (int): Frequency to print metrics (in epochs).\n",
        "        Returns:\n",
        "            losses (list): The list of training losses for each epoch.\n",
        "            accuracies (list): The list of validation accuracy values for each epoch.\n",
        "        \"\"\"\n",
        "        num_batches_per_epoch = len(X_train) // batch_size\n",
        "        do_validation = X_val is not None and y_val is not None\n",
        "        losses, accuracies = [], []\n",
        "        for i in range(num_epochs): # for each training epoch\n",
        "            epoch_loss = 0\n",
        "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
        "                # Get batch:\n",
        "                batch_index_begin = b * batch_size\n",
        "                batch_index_end = batch_index_begin + batch_size\n",
        "                x = X_train[batch_index_begin: batch_index_end]\n",
        "                targets = y_train[batch_index_begin: batch_index_end]\n",
        "                # Optimize on batch:\n",
        "                predictions = y = self.forward(x)  # forward pass\n",
        "                L = self.loss_function(predictions, targets)  # loss computation\n",
        "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
        "                self.backward(dL_dy)  # back-propagation pass\n",
        "                self.optimize(learning_rate)  # optimization of the NN\n",
        "                epoch_loss += L\n",
        "\n",
        "            # Logging training loss and validation accuracy, to follow the training:\n",
        "            epoch_loss /= num_batches_per_epoch\n",
        "            losses.append(epoch_loss)\n",
        "            if do_validation:\n",
        "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
        "                accuracies.append(accuracy)\n",
        "            else:\n",
        "                accuracy = np.NaN\n",
        "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
        "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
        "                    i, epoch_loss, accuracy * 100))\n",
        "        return losses, accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFCcQevDZAvb"
      },
      "source": [
        "***Note:*** This class can also be found in [simple_network.py](simple_network.py).\n"
      ]
    }
  ]
}